{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "import datetime\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Keras 1.2.2 import\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape, ActivityRegularization\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Convolution3D, MaxPooling3D\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD, Adagrad, Adam\n",
    "from keras.regularizers import l1\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Functions Definition:\n",
    "\n",
    "# Read data from csv file function\n",
    "def read_data(file_path):\n",
    "    #column_names = ['user','timestamp', 'x-axis', 'y-axis', 'z-axis', 'label']\n",
    "    data = pd.read_csv(file_path, header = 0, sep = ',')#, names = column_names)\n",
    "    return data\n",
    "\n",
    "def tail(f, n):\n",
    "    assert n >= 0\n",
    "    pos, lines = n+1, []\n",
    "    while len(lines) <= n:\n",
    "        try:\n",
    "            f.seek(-pos, 2)\n",
    "        except IOError:\n",
    "            f.seek(0)\n",
    "            break\n",
    "        finally:\n",
    "            lines = list(f)\n",
    "        pos *= 2\n",
    "    return lines[-n:]\n",
    "\n",
    "def time_el(pre):\n",
    "    \n",
    "    now = datetime.datetime.now() # Today complete date and time\n",
    "    \n",
    "    return ((now.day - pre[0])*24*60) + ((now.hour - pre[1])*60) + (now.minute - pre[2])\n",
    "\n",
    "def size_keras(model): # Compute number of params in a model (the actual number of floats)\n",
    "    return sum([np.prod(K.get_value(w).shape) for w in model.trainable_weights])\n",
    "\n",
    "def save_result_line(filename, results):\n",
    "\n",
    "    # Getting last used ID number\n",
    "    f = open(filename)\n",
    "    last_line = tail(f, 1)\n",
    "    last_results = np.genfromtxt(last_line, delimiter='\\t')\n",
    "    f.close\n",
    "    \n",
    "    if last_line == []:\n",
    "        results[0] = 1\n",
    "    else:\n",
    "        results[0] = last_results[0] + 1\n",
    "    \n",
    "    # Saving current result line\n",
    "    f = open(filename, 'ab') \n",
    "    np.savetxt(f, np.reshape(results, (1,-1)), delimiter='\\t')\n",
    "    f.close()\n",
    "    \n",
    "    return results[0]\n",
    "\n",
    "def loss_rank_one(yt, yp, margin_value):\n",
    "    \n",
    "    s = yt * 1\n",
    "    m1 = np.argmax(s)\n",
    "    s[m1] = 0\n",
    "    m2 = np.argmax(s)\n",
    "    \n",
    "    max1 = np.ones(len(yt)) * yp[m1]\n",
    "    max2 = np.ones(len(yt)) * yp[m2]\n",
    "    margin = np.ones(len(yt)) * margin_value\n",
    "    \n",
    "    loss = 2 * np.sum(np.log(1 + np.exp(-(max1 - yp - margin))))\n",
    "    loss += np.sum(np.log(1 + np.exp(-(max2 - yp - margin))))\n",
    "    loss += - np.sum(np.log(1 + np.exp(-(yp[m2] - yp[m1] - margin_value))))\n",
    "    \n",
    "    return loss\n",
    "                \n",
    "def loss_rank(yt_list, yp_list, margin_value):\n",
    "    \n",
    "    m = yp_list.shape[0]\n",
    "    C_loss = 0\n",
    "    \n",
    "    for jj in range(0, m):\n",
    "        C_loss = C_loss + loss_rank_one(yt_list[jj, :], yp_list[jj, :], margin_value)\n",
    "        \n",
    "    C_loss = (C_loss / m)\n",
    "\n",
    "    return C_loss\n",
    "\n",
    "# For multiple samples (numpy 2D array input)\n",
    "def loss_keras_margin(margin_value):\n",
    "    def loss_keras(yt, yp):\n",
    "\n",
    "        # 1st most dominant - Guessing 1st dominant is more important\n",
    "        ind_max = K.argmax(yt, 1)\n",
    "        flat_y = K.reshape(yp, [-1])  # Reshape to a vector.\n",
    "\n",
    "        flat_ind_max = ind_max + K.cast(tf.range(tf.shape(yp)[0]) * tf.shape(yp)[1], tf.int64)\n",
    "        y_m1 = K.reshape(K.tf.gather(flat_y, flat_ind_max), [-1, 1])\n",
    "        \n",
    "        y_mm1 = K.repeat_elements(y_m1, K.int_shape(yp)[1], 1)\n",
    "\n",
    "        # 2nd most dominant\n",
    "        ind_min = K.argmin(yt, 1)\n",
    "        flat_y = K.reshape(yp, [-1])  # Reshape to a vector.\n",
    "\n",
    "        flat_ind_min = ind_min + K.cast(tf.range(tf.shape(yp)[0]) * tf.shape(yp)[1], tf.int64)\n",
    "        y_m2 = K.reshape(K.tf.gather(flat_y, flat_ind_max), [-1, 1])\n",
    "        y_mm2 = K.repeat_elements(y_m2, K.int_shape(yp)[1], 1)\n",
    "\n",
    "        \n",
    "        margin = (y_mm2/y_mm2) * margin_value\n",
    "\n",
    "        loss = (2 * K.sum(K.mean(K.log(1 + K.exp(-(y_mm1 - yp - margin))), 0)) \\\n",
    "                + K.sum(K.mean(K.log(1 + K.exp(-(y_mm2 - yp - margin))),0)) \\\n",
    "                - K.sum(K.mean(K.log(1 + K.exp(-(y_m2 - y_m1 - margin[:,0]))),0)))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    return loss_keras # Returns loss function it self\n",
    "\n",
    "def dcg_at_k(r, k, method=0):\n",
    "\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        if method == 0:\n",
    "            return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
    "        elif method == 1:\n",
    "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        else:\n",
    "            raise ValueError('method must be 0 or 1.')\n",
    "    return 0.\n",
    "\n",
    "\n",
    "def eval_custom(yt, ypp, margin_value):\n",
    "    \n",
    "    # If it is baseline, copy one vector to all spots, making yt same size as yp\n",
    "    if len(ypp) == len(yt) :\n",
    "        yp = ypp\n",
    "    else :\n",
    "        yp = []\n",
    "        for i_h in range(0,len(yt)):\n",
    "            yp.append(ypp)\n",
    "        yp = np.array(yp)\n",
    "    \n",
    "    # KLD metric\n",
    "    kld = 0.0\n",
    "    for ii in range(0, len(yt)):\n",
    "        #temp = yt[ii, :] != 0 # This eliminates points where yt is predicted to be 0\n",
    "        #kld = kld + (np.sum(stats.entropy(yt[ii, temp], yp[ii, temp])) / len(yt))\n",
    "        kld = kld + (np.sum(stats.entropy(yt[ii, :], yp[ii, :])) / len(yt))\n",
    "    \n",
    "    # 1st dominant accuracy\n",
    "    dom1 = np.sum((np.argmax(yt,1) == np.argmax(yp,1))*1.0) /  len(yt)\n",
    "    \n",
    "    # 1st+2nd dominant accuracy\n",
    "    st = yt * 1\n",
    "    sp = yp * 1\n",
    "    for ii in range(0, len(yt)):\n",
    "        st[ii][ np.argmax(st[ii,:]) ] = 0\n",
    "        sp[ii][ np.argmax(sp[ii,:]) ] = 0\n",
    "    dom12 = np.sum(((np.argmax(st,1) == np.argmax(sp,1)) & (np.argmax(yt,1) == np.argmax(yp,1)))*1.0) /  len(yt)\n",
    "    \n",
    "    # Discounted Cumulative Gain\n",
    "    dcg = 0\n",
    "    for ii in range(0, len(yt)):\n",
    "        s = np.argsort(yt[ii,:])\n",
    "        sp = np.zeros(yt.shape[1])\n",
    "        for jj in range(0, yt.shape[1]):\n",
    "            sp[jj] = yp[ii, s[jj]]\n",
    "        dcg = dcg + (dcg_at_k(sp, yt.shape[1]) / len(yt))\n",
    "    \n",
    "    # Logaritmic rank loss\n",
    "    rank = loss_rank(yt, yp, margin_value)\n",
    "    \n",
    "    return [kld, dom1, dom12, dcg, rank]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keras model definition function\n",
    "def new_model(x_dim, classes, reg, l_rate, k_mod, obj, margin_value):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    if k_mod == 0 : # -> 'linear'\n",
    "        model.add(Flatten(input_shape=(x_dim[0],x_dim[1],1)))\n",
    "        model.add(Dense(classes, activation = 'softmax', W_regularizer = l1(reg), init='he_normal'))\n",
    "    \n",
    "    if k_mod == 1 : # -> 'dnn_1'\n",
    "        model.add(Flatten(input_shape=(x_dim[0],x_dim[1],1)))\n",
    "        model.add(Dense(50, activation = 'relu', W_regularizer = l1(0.001), init='he_normal'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(classes, activation = 'softmax', W_regularizer = l1(reg), init='he_normal'))\n",
    "    \n",
    "    if k_mod == 2 : # -> 'dnn_2'\n",
    "        model.add(Flatten(input_shape=(x_dim[0],x_dim[1],1)))\n",
    "        model.add(Dense(1000, activation = 'relu', W_regularizer = l1(0.001), init='he_normal'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(50, activation = 'relu', W_regularizer = l1(0.001), init='he_normal'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(classes, activation = 'softmax', W_regularizer = l1(reg), init='he_normal'))\n",
    "    \n",
    "    if k_mod == 3 : # -> 'dnn_3'\n",
    "        model.add(Flatten(input_shape=(x_dim[0],x_dim[1],1)))\n",
    "        model.add(Dense(100, activation = 'relu', W_regularizer = l1(0.001), init='he_normal'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(100, activation = 'relu', W_regularizer = l1(0.001), init='he_normal'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(50, activation = 'relu', W_regularizer = l1(0.001), init='he_normal'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(classes, activation = 'softmax', W_regularizer = l1(reg), init='he_normal'))\n",
    "    \n",
    "    if k_mod == 5 : # -> 'cnn_1'\n",
    "        model.add(Convolution2D(32, 3, 3, activation='relu', border_mode='same', \n",
    "                                input_shape=(x_dim[0],x_dim[1],1)))\n",
    "        model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(50, activation='relu', W_regularizer = l1(reg), init='he_normal'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(classes, activation='softmax', W_regularizer = l1(reg), init='he_normal'))\n",
    "\n",
    "    if k_mod == 6 : # -> 'cnn_2'\n",
    "        model.add(Convolution2D(32, 3, 3, activation='relu',  border_mode='same',\n",
    "                                input_shape=(x_dim[0], x_dim[1], 1)))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Convolution2D(16,3 ,3 , activation='relu', border_mode='same', ))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Convolution2D(16, 3, 3, activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(50, activation='relu', W_regularizer = l1(0.001), init='he_normal'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(classes, activation='softmax', W_regularizer = l1(reg), init='he_normal'))\n",
    "    \n",
    "    if k_mod == 7 : # -> 'cnn_3D'\n",
    "        # Only for raw input x data, it needs to be reshaped\n",
    "        model.add(Reshape((x_dim[2], x_dim[3], x_dim[4], 1), input_shape=(x_dim[0], x_dim[1], 1)))\n",
    "        model.add(Convolution3D(32, 3, 3, 3, activation='relu',  border_mode='same',))\n",
    "        model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "        model.add(Convolution3D(16, 3 ,3, 3, activation='relu', border_mode='same', ))\n",
    "        model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "        model.add(Convolution3D(8, 3, 3, 3, activation='relu'))\n",
    "        model.add(MaxPooling3D(pool_size=(2,2,2)))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(50, activation='relu', W_regularizer = l1(0.001), init='he_normal'))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(classes, activation='softmax', W_regularizer = l1(reg), init='he_normal'))\n",
    "    \n",
    "    # Compilation of model\n",
    "    if obj < 2:\n",
    "        model.compile(loss = 'kullback_leibler_divergence',\n",
    "                  optimizer = Adam(lr=l_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                  metrics = ['kullback_leibler_divergence'])\n",
    "    \n",
    "    if obj == 2:\n",
    "        model.compile(loss = 'categorical_crossentropy',\n",
    "                  optimizer = Adam(lr=l_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0),\n",
    "                  metrics = ['accuracy'])\n",
    "        \n",
    "    if obj == 3:\n",
    "        model.compile(loss = loss_keras_margin(margin_value), \\\n",
    "                  optimizer = Adam(lr=l_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0), \\\n",
    "                  metrics = ['kullback_leibler_divergence'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_run(n_epoch, av_epoch, random_set, cut, nnz_01, l_rate, reg_lambda, k_mod, obj, inp, m_v, prin_t):\n",
    "    \n",
    "    # Starting timer, to measure how long it takes to generate this one run\n",
    "    tim_start = timer()\n",
    "\n",
    "    # Output variable declaration\n",
    "    metadata = 20\n",
    "    n_metric = 5 # (KLD, 1st dom, 1+2nd dom, DCG, Log loss 1, Log l. 2, Log l. 3, Log l. 4, Log l. 5, reserve)\n",
    "    baseline = 4 * n_metric\n",
    "    results = np.zeros(metadata + baseline + int(((n_epoch/av_epoch)*2*n_metric)))\n",
    "    \n",
    "    ## Saving Meta-Data to current Results line\n",
    "    # Time information\n",
    "    now = datetime.datetime.now() # Today complete date and time\n",
    "    results[1] = now.year\n",
    "    results[2] = now.month\n",
    "    results[3] = now.day\n",
    "    results[4] = now.hour\n",
    "    results[5] = now.minute\n",
    "    #results[6] will be finally filled with elapsed time in minutes\n",
    "    #results[7] will be finally filled with elapsed time from timer\n",
    "    #results[8] will be finally filled with total number of parameters in used keras model\n",
    "    \n",
    "    # If the objective is not our custom rank loss function, set margin to 0\n",
    "    if obj != 3:\n",
    "        m_v = 0\n",
    "    \n",
    "    # Given settings\n",
    "    results[9:20] = [n_epoch, av_epoch, random_set, cut, (nnz_01*1), l_rate, reg_lambda, k_mod, obj, inp, m_v]\n",
    "    \n",
    "    ## ------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Load data from files\n",
    "    if (inp == 0) | (inp == 2): # Means Histogram input is selected\n",
    "        # Borders to cut Histograms\n",
    "        border_cut = read_data('IN_borders.csv')\n",
    "        border = np.array(border_cut[0:])  # Casting as numpy array\n",
    "        # Input Histograms\n",
    "        inp_hist = read_data('IN_histogram.csv')\n",
    "        data_inp = np.array(inp_hist[0:]) * 1.0 # Making sure that our array is float type (no integer)\n",
    "        data_inp = data_inp[0:-1] # One data point is discarded to make processing easier (110/5 = 22)\n",
    "        # Output labels\n",
    "        out_labels = read_data('IN_labels.csv') # Output true labels\n",
    "        data_out_raw = np.array(out_labels)  # Casting as numpy array\n",
    "        data_out_raw = data_out_raw[0:-1] # One data point is discarded to make processing easier (110/5 = 22)\n",
    "    else:\n",
    "        # Input Raw 3D data\n",
    "        f = open('IN_labels_raw_input.dat')\n",
    "        data_out_raw = np.genfromtxt(f, delimiter='\\t')\n",
    "        f.close\n",
    "        # Input Raw 3D data\n",
    "        f = open('IN_raw_input.dat')\n",
    "        data_inp = np.genfromtxt(f, delimiter='\\t')\n",
    "        f.close\n",
    "        data_inp = np.reshape(data_inp, (-1, data_inp.shape[1], 1, 1))\n",
    "        cut = -1 # To prevent cutting the input data\n",
    "        \n",
    "    # Reducing labels to 3 classes (when: inp = 2, inp = 3)\n",
    "    if inp > 1:\n",
    "        temp = data_out_raw * 1.0\n",
    "        data_out_raw = np.zeros([temp.shape[0], 3])\n",
    "        data_out_raw[:,0] = temp[:,0]\n",
    "        data_out_raw[:,1] = temp[:,1] + temp[:,2]\n",
    "        data_out_raw[:,2] = temp[:,3] + temp[:,4]\n",
    "        # Renormalize to 1\n",
    "        for ii in range(0, data_out_raw.shape[0]):\n",
    "            data_out_raw[ii, :] = data_out_raw[ii, :] / np.sum(data_out_raw[ii, :])\n",
    "    \n",
    "    ## Processing labels for different objectives\n",
    "    # No change to output labels\n",
    "    if obj == 0:\n",
    "        data_out = data_out_raw * 1\n",
    "    \n",
    "    # Keep just 1st most dominant type with value 1 (1 for 1st dominant, zero elsewhere)\n",
    "    if obj == 1:\n",
    "        temp = np.argmax(data_out_raw, axis=-1)\n",
    "        data_out = np_utils.to_categorical(temp, data_out_raw.shape[1])\n",
    "        \n",
    "    # Keep just 1st and 2nd dominant type, renormalize to 1 ([0|0.2|0.1|0.15|0.5|0.05]->[0|0.2|0|0|0.5|0]/0.7)\n",
    "    if obj == 2:\n",
    "        temp = data_out_raw * 1.0\n",
    "        max1 = np.argmax(temp, 1)\n",
    "        for jj in range(0, temp.shape[0]):\n",
    "            temp[jj, max1[jj]] = 0\n",
    "        max2 = np.argmax(temp, 1)\n",
    "        data_out = data_out_raw * 0.0\n",
    "        for jj in range(0, data_out.shape[0]):\n",
    "            data_out[jj, max1[jj]] = data_out_raw[jj, max1[jj]]\n",
    "            data_out[jj, max2[jj]] = data_out_raw[jj, max2[jj]]\n",
    "            data_out[jj, :] = data_out[jj, :] / np.sum(data_out[jj, :])\n",
    "        \n",
    "    # Create special ranking vector for my custom loss function (1st = 1, 2nd = -1, else = 0)\n",
    "    if obj == 3:\n",
    "        temp = data_out_raw * 1.0\n",
    "        max1 = np.argmax(temp, 1)\n",
    "        for jj in range(0, temp.shape[0]):\n",
    "            temp[jj, max1[jj]] = 0\n",
    "        max2 = np.argmax(temp, 1)\n",
    "        data_out = temp * 0.0\n",
    "        for jj in range(0, temp.shape[0]):\n",
    "            data_out[jj, max1[jj]] = 1\n",
    "        for jj in range(0, temp.shape[0]):\n",
    "            data_out[jj, max2[jj]] = -1\n",
    "    \n",
    "    ## Processing input\n",
    "    # Choose the compression (Cutting the input)\n",
    "    if (cut <= 100) & (cut > 0) : # If Raw input is used, cut variable is set to -1\n",
    "        condition = border >= cut\n",
    "        correct_borders = border[condition[:,-1], :]\n",
    "        bcut = correct_borders[-1,0:4]\n",
    "        data_inp = np.reshape(data_inp, (-1, int(border[0,1]), int(border[0,3]), 1))\n",
    "        data_inp = data_inp[:, bcut[0]:bcut[1], bcut[2]:bcut[3], :]\n",
    "\n",
    "    # Useful variables\n",
    "    x_dim = np.zeros(5, dtype=np.int32)\n",
    "    x_dim[0:2] = [data_inp.shape[1], data_inp.shape[2]]\n",
    "    if (inp == 1) | (inp == 3): # Means Raw input is selected\n",
    "        x_dim[2:5] = [70, 83, 80] # Size of raw input (70x83x40 for iodine and vnc)\n",
    "    samples = data_out.shape[0]\n",
    "    classes = data_out.shape[1]\n",
    "    set_size = (samples / 5) # There are 5 CV sets\n",
    "        \n",
    "    # Replacing all Non-Zero elements with 1\n",
    "    if nnz_01 == True :\n",
    "        data_inp = np.array(data_inp, dtype=bool) * 1.0\n",
    "    \n",
    "    # Shuffle of data segments\n",
    "    data_inp, data_out = shuffle(data_inp, data_out, random_state = int(random_set)) # Pseudo-random shaffling\n",
    "    \n",
    "    # Dataset split to 5 mini sets\n",
    "    X_split = np.zeros((5, set_size, x_dim[0], x_dim[1], 1))\n",
    "    y_split = np.zeros((5, set_size, classes))\n",
    "    y_split_raw = np.zeros((5, set_size, classes))\n",
    "    for i_split in range(0, 5):\n",
    "        X_split[i_split, :, :, :] = data_inp[i_split*set_size:set_size+i_split*set_size, :, :, :]\n",
    "        y_split[i_split, :, :] = data_out[i_split*set_size:set_size+i_split*set_size, :]\n",
    "        y_split_raw[i_split, :, :] = data_out_raw[i_split*set_size:set_size+i_split*set_size, :]\n",
    "\n",
    "    # Variables for final model training\n",
    "    Xa = np.zeros((set_size*4, x_dim[0], x_dim[1], 1)) # Xa is moved to X during normalization,\n",
    "    X = np.zeros((set_size*4, x_dim[0], x_dim[1], 1))  # then Xa is not used\n",
    "    y = np.zeros((set_size*4, classes))\n",
    "     \n",
    "    Xa_test = np.zeros((set_size, x_dim[0],x_dim[1], 1)) \n",
    "    X_test = np.zeros((set_size, x_dim[0],x_dim[1], 1))\n",
    "    y_test = np.zeros((set_size, classes))\n",
    "    \n",
    "    # Raw labels to calculate metrics correctly, when different label is used for training \n",
    "    y_raw = np.zeros((set_size*4, classes))\n",
    "    y_test_raw = np.zeros((set_size, classes))\n",
    "    \n",
    "    # Cross validation execution over 5 volts\n",
    "    for i_5set in range(0, 5):\n",
    "        Xa[0:set_size, :, :] = X_split[i_5set % 5, :, :, :]\n",
    "        Xa[set_size:set_size*2, :, :] = X_split[(i_5set+1) % 5, :, :, :]\n",
    "        Xa[set_size*2:set_size*3, :, :] = X_split[(i_5set+2) % 5, :, :, :]\n",
    "        Xa[set_size*3:, :, :] = X_split[(i_5set+3) % 5, :, :, :]\n",
    "\n",
    "        Xa_test = X_split[(i_5set+4) % 5, :, :, :]\n",
    "\n",
    "        y[0:set_size, :] = y_split[i_5set % 5, :, :]\n",
    "        y[set_size:set_size*2, :] = y_split[(i_5set+1) % 5, :, :]\n",
    "        y[set_size*2:set_size*3, :] = y_split[(i_5set+2) % 5, :, :]\n",
    "        y[set_size*3:, :] = y_split[(i_5set+3) % 5, :, :]\n",
    "\n",
    "        y_test = y_split[(i_5set+4) % 5, :, :]\n",
    "        \n",
    "        y_raw[0:set_size, :] = y_split_raw[i_5set % 5, :, :]\n",
    "        y_raw[set_size:set_size*2, :] = y_split_raw[(i_5set+1) % 5, :, :]\n",
    "        y_raw[set_size*2:set_size*3, :] = y_split_raw[(i_5set+2) % 5, :, :]\n",
    "        y_raw[set_size*3:, :] = y_split_raw[(i_5set+3) % 5, :, :]\n",
    "\n",
    "        y_test_raw = y_split_raw[(i_5set+4) % 5, :, :]\n",
    "\n",
    "        # Normalization 1-Zero mean + Unit variance (if nnz_01 is true, than SKIPPED)\n",
    "        if nnz_01 == False :\n",
    "            st_dev = np.std(Xa, axis=0)\n",
    "            st_dev[st_dev == 0] = 1\n",
    "\n",
    "            mean = np.mean(Xa, axis=0)\n",
    "\n",
    "            for i_norm1 in range(0, X.shape[0]):\n",
    "                X[i_norm1, :, :] = np.divide(Xa[i_norm1, :, :] - mean, st_dev)\n",
    "\n",
    "            for i_norm2 in range(0, X_test.shape[0]):\n",
    "                X_test[i_norm2, :, :] = np.divide(Xa_test[i_norm2, :, :] - mean, st_dev)\n",
    "        \n",
    "        if nnz_01 == True :\n",
    "            X_test = Xa_test\n",
    "            X = Xa\n",
    "        \n",
    "        # Zero-line evaluation (uniform distribution)\n",
    "        model_prob = np.array(np.ones(classes) * (1/classes))\n",
    "        results[metadata:metadata+n_metric] += (np.array(eval_custom(y_raw, model_prob,m_v)) / 5)\n",
    "        results[metadata+n_metric:metadata+(n_metric*2)]+=(np.array(eval_custom(y_test_raw,model_prob,m_v)) / 5)\n",
    "        \n",
    "        # Baseline evaluation (averaging the training set)\n",
    "        model_prob = np.mean(y_raw, axis=0)\n",
    "        results[metadata+(n_metric*2):metadata+(n_metric*3)] += (np.array(eval_custom(y_raw,model_prob,m_v))/5)\n",
    "        results[metadata+(n_metric*3):metadata+baseline] += (np.array(eval_custom(y_test_raw,model_prob,m_v))/5)\n",
    "        \n",
    "        # -------------------------Keras execution ---------------------------------\n",
    "\n",
    "        # Creation of a new model (viz new_model function)\n",
    "        model = new_model(x_dim, classes, reg_lambda, l_rate, k_mod, obj, margin_value = m_v)\n",
    "\n",
    "        for i_epoch in range(0, int(n_epoch/av_epoch)):\n",
    "            \n",
    "            # Training the model\n",
    "            mod_t = model.fit(X, y, batch_size=32, nb_epoch=int(av_epoch), verbose=0)\n",
    "            \n",
    "            rfi = metadata + baseline + (i_epoch * 2 * n_metric) # current epoch results first index\n",
    "            \n",
    "            # Evaluating this model on training data\n",
    "            results[rfi:rfi+n_metric] += (np.array(eval_custom(y_raw,model.predict_proba(X, verbose=0),m_v))/5)\n",
    "\n",
    "            # Evaluating this model on validation data\n",
    "            results[rfi+n_metric:rfi+2*n_metric] += \\\n",
    "                            (np.array(eval_custom(y_test_raw, model.predict_proba(X_test, verbose=0),m_v))/5)\n",
    "                \n",
    "        if prin_t :\n",
    "            print (i_5set+1), \" / 5\" # Progress print whithin one run\n",
    "    \n",
    "    tim_end = timer()\n",
    "    results[6] = time_el(results[3:6])\n",
    "    results[7] = tim_end - tim_start\n",
    "    results[8] = size_keras(model)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Settings --------------------------------------------------------\n",
    "\n",
    "# List of available models and objectives\n",
    "keras_model_dict = {'linear': 0,     # Just output softmax layer with 6 nodes\n",
    "                    'dnn_1': 1,\n",
    "                    'dnn_2': 2,\n",
    "                    #'dnn_3': 3,\n",
    "                    #'dnn_4': 4,\n",
    "                    'cnn_1': 5,\n",
    "                    'cnn_2': 6,\n",
    "                    'cnn_3D': 7,\n",
    "                    #'cnn_4': 8,\n",
    "                    'cnn_5': 9}\n",
    "objective_dict = {'kld': 0, '1+2': 1, '1st': 2, 'rank': 3}\n",
    "\n",
    "# Should be fixed to keep size of the output matrix (at least in terms of one Results file)\n",
    "number_of_epoch            = 5000     # int......any number\n",
    "evaluate_each_x_epoch      = 2        # Evaluation will be done after x epochs (must be denominator of n_epoch)\n",
    "\n",
    "# Hyperparameters to TUNE\n",
    "random_set                 = 20       # int......any number (should not be changed across different methods)\n",
    "histogram_dimension_cut    = 100      # Options: 100 95 90 80 70 60 50 40 30 20 (100 -> no cut -> 6615 pixels)\n",
    "making_histogram_values_01 = False    # If set to true, all histogram non-zero values are set to 1, 0 kept as 0) \n",
    "learning_rate              = 0.001    # float....learning_rate > 0 (Tested best value: 0.001)\n",
    "regularization_lambda      = 0.001      # float....reg_lambda > 0 (for output softmax layer)\n",
    "\n",
    "# Keras model Settings\n",
    "keras_model                = 'cnn_3D' # Model selection: 'linear'   'dnn'   'cnn'\n",
    "kms = []   # Empty list               # Each settings entry (settings for one type of model) must have size 20!\n",
    "objective                  = 'kld'   # Model will either fit KLD, or classify 1st dominant, or rank 1,2nd\n",
    "use_raw_input              = True    # Make true, if you want to use raw input data instead of histogram\n",
    "use_3_classes              = False    # Reduce output label to 3 classes\n",
    "margin_value               = 0     # Margin value to custom loss function (value 0 ~ 1) (ignore if other loss)\n",
    "\n",
    "## Execution -------------------------------------------------------\n",
    "\n",
    "# Getting results for one specified configuration                \n",
    "results = one_run(n_epoch = number_of_epoch,\n",
    "                  av_epoch = evaluate_each_x_epoch,\n",
    "                  \n",
    "                  random_set = random_set,             \n",
    "                  cut = histogram_dimension_cut,\n",
    "                  nnz_01 = making_histogram_values_01,\n",
    "                  l_rate = learning_rate,\n",
    "                  reg_lambda = regularization_lambda,\n",
    "                  \n",
    "                  k_mod = keras_model_dict[keras_model],\n",
    "                  obj = objective_dict[objective],\n",
    "                  inp = (use_raw_input * 1) + (use_3_classes * 2),\n",
    "                  m_v = margin_value,\n",
    "                  \n",
    "                  prin_t = True\n",
    "                  )\n",
    "\n",
    "# Save results to .dat file\n",
    "cur_id = save_result_line('Results_U2.dat', results) # It will append the file (with current ID number)\n",
    "\n",
    "# Done statement\n",
    "print 'All DONE!  ->  Results saved with date: ', results[1], '-', results[2], '-', results[3], '  ID: ', cur_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Using preset combinations from external file\n",
    "\n",
    "# Loading settings file\n",
    "f = open('prm_config.dat')\n",
    "set_list = np.genfromtxt(f, delimiter='\\t')\n",
    "f.close\n",
    "\n",
    "# Execution -------------------------------------------------------\n",
    "for i_combination in range(0, set_list.shape[0]):\n",
    "\n",
    "    sett = set_list[i_combination, :]\n",
    "\n",
    "    results = one_run(n_epoch = sett[0],\n",
    "                      av_epoch = sett[1],\n",
    "\n",
    "                      random_set = sett[2],             \n",
    "                      cut = sett[3],\n",
    "                      nnz_01 = (1 == sett[4]),\n",
    "                      l_rate = sett[5],\n",
    "                      reg_lambda = sett[6],\n",
    "\n",
    "                      k_mod = sett[7],\n",
    "                      obj = sett[8],\n",
    "                      inp = sett[9],\n",
    "                      m_v = sett[10],\n",
    "                      \n",
    "                      prin_t = False\n",
    "                      )\n",
    "\n",
    "    # Save results to .dat file\n",
    "    cur_id = save_result_line('Results_U2.dat', results) # It will append the file (with current ID number)\n",
    "    \n",
    "    # Partial done statement\n",
    "    print 'All DONE!  ->  Results saved with date: ', results[1],'-',results[2],'-',results[3], '  ID: ', cur_id\n",
    "\n",
    "# Done statement\n",
    "print 'All DONE!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
